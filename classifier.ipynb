{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# %env CUDA_VISIBLE_DEVICES=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 22:00:42.395479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-28 22:00:43.097780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn import preprocessing\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"retriever/data/train_val_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data['folds'][0]['train'])\n",
    "val_df = pd.DataFrame(data['folds'][0]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.classifier_1 != \"–û–¢–°–£–¢–°–¢–í–£–ï–¢\"]\n",
    "val_df = val_df[val_df.classifier_1 != \"–û–¢–°–£–¢–°–¢–í–£–ï–¢\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "train_df['labels'] = le.fit_transform(train_df.classifier_2.values)\n",
    "val_df['labels'] = le.fit_transform(val_df.classifier_2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = dict(zip(train_df.classifier_2, train_df.labels))\n",
    "id2label = dict(zip(train_df.labels, train_df.classifier_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34: '–£–¥–∞–ª–µ–Ω–∏–µ –∞–∫–∫–∞—É–Ω—Ç–∞',\n",
       " 14: '–û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ/–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –≤–∏–¥–µ–æ',\n",
       " 15: '–û—Ç–∫–ª—é—á–µ–Ω–∏–µ/–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏',\n",
       " 6: '–ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ',\n",
       " 11: '–ù–∞–≤–∏–≥–∞—Ü–∏—è',\n",
       " 2: '–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è',\n",
       " 23: '–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ',\n",
       " 36: '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–µ–π',\n",
       " 20: '–ü–ª–µ–µ—Ä',\n",
       " 24: '–ü—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–∞',\n",
       " 35: '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–ª–µ–µ—Ä–æ–º',\n",
       " 17: '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è',\n",
       " 10: '–ú–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è',\n",
       " 28: '–°–º–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏/–≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è',\n",
       " 26: '–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è/–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è',\n",
       " 4: '–í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ',\n",
       " 16: '–ü–µ—Ä–µ–Ω–æ—Å –≤–∏–¥–µ–æ —Å Youtube',\n",
       " 3: '–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–∏–¥–µ–æ',\n",
       " 12: '–ù–∞—Ä—É—à–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –ø—Ä–∞–≤',\n",
       " 5: '–î–æ–ª–≥–∞—è –º–æ–¥–µ—Ä–∞—Ü–∏—è',\n",
       " 33: '–¢—Ä–∞–Ω—Å–ª—è—Ü–∏—è',\n",
       " 29: '–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏',\n",
       " 32: '–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫',\n",
       " 13: '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ',\n",
       " 1: '–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∫–∞–Ω–∞–ª–∞',\n",
       " 8: '–ò—Å—Ç–æ—Ä–∏—è –ø–æ–∏—Å–∫–∞',\n",
       " 30: '–°—Ç—É–¥–∏—è RUTUBE',\n",
       " 27: '–°–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π',\n",
       " 25: '–ü—Ä–æ—Å–º–æ—Ç—Ä —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏',\n",
       " 7: '–ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç',\n",
       " 19: '–ü–ª–∞—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç',\n",
       " 21: '–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–Ω–∞—Ç–æ–≤',\n",
       " 0: '–ê–Ω–∞–ª–∏—Ç–∏–∫–∞',\n",
       " 22: '–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∫–ª–∞–º—ã',\n",
       " 9: '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏',\n",
       " 18: '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è 0',\n",
       " 31: '–¢–í-—ç—Ñ–∏—Ä—ã',\n",
       " 37: '–ß–∞—Ç/–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_classes = train_df['classifier_2'].value_counts().tail(21).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>classifier_1</th>\n",
       "      <th>classifier_2</th>\n",
       "      <th>is_knowledge_base</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>–í –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –ø–æ—è–≤—è—Ç—Å—è —Å—É–±—Ç–∏—Ç—Ä—ã?</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç, –∞ –±—É–¥—É—Ç –ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>–î–æ–±—Ä—ã–π –¥–µ–Ω—å, –ø–æ—è–≤—è—Ç—Å—è –ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞...</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>–ë—É–¥—É—Ç —Å—É–±—Ç–∏—Ç—Ä—ã –Ω–∞ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>–ö–æ–≥–¥–∞ –¥–æ–±–∞–≤–∏—Ç–µ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π? –≠...</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>–ê —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –±—É–¥—É—Ç?</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>–°–∫–∞–∂–∏—Ç–µ, –ø–ª–∞–Ω–∏—Ä—É—é—Ç—Å—è –ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –¥–ª—è –∑–∞–ø–∏—Å–µ–π —Ç...</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>–ü–æ—è–≤—è—Ç—Å—è –ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>–ï—Å—Ç—å –ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–∏—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –∫ –∑–∞–ø–∏—Å–∏...</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>–°—É–±—Ç–∏—Ç—Ä—ã –±—É–¥—É—Ç –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π?</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>–ö–æ–≥–¥–∞ –º–æ–∂–Ω–æ –æ–∂–∏–¥–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?</td>\n",
       "      <td>–°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>–î–æ—Å—Ç—É–ø–µ–Ω –ª–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —Å—É–±—Ç–∏—Ç—Ä–æ–≤ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?</td>\n",
       "      <td>–ü–æ–∫–∞ —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã —É—á—Ç–µ–º –≤–∞—à–µ –ø—Ä–µ–¥–ª...</td>\n",
       "      <td>–ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø</td>\n",
       "      <td>–ü–ª–µ–µ—Ä</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "143             –í –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –ø–æ—è–≤—è—Ç—Å—è —Å—É–±—Ç–∏—Ç—Ä—ã?   \n",
       "144   –ü—Ä–∏–≤–µ—Ç, –∞ –±—É–¥—É—Ç –ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?   \n",
       "145  –î–æ–±—Ä—ã–π –¥–µ–Ω—å, –ø–æ—è–≤—è—Ç—Å—è –ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞...   \n",
       "146               –ë—É–¥—É—Ç —Å—É–±—Ç–∏—Ç—Ä—ã –Ω–∞ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?   \n",
       "147  –ö–æ–≥–¥–∞ –¥–æ–±–∞–≤–∏—Ç–µ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π? –≠...   \n",
       "148              –ê —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –±—É–¥—É—Ç?   \n",
       "149  –°–∫–∞–∂–∏—Ç–µ, –ø–ª–∞–Ω–∏—Ä—É—é—Ç—Å—è –ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –¥–ª—è –∑–∞–ø–∏—Å–µ–π —Ç...   \n",
       "150          –ü–æ—è–≤—è—Ç—Å—è –ª–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?   \n",
       "151  –ï—Å—Ç—å –ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–∏—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –∫ –∑–∞–ø–∏—Å–∏...   \n",
       "152                –°—É–±—Ç–∏—Ç—Ä—ã –±—É–¥—É—Ç –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π?   \n",
       "153  –ö–æ–≥–¥–∞ –º–æ–∂–Ω–æ –æ–∂–∏–¥–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –≤ –∑–∞–ø–∏—Å–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?   \n",
       "854       –î–æ—Å—Ç—É–ø–µ–Ω –ª–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —Å—É–±—Ç–∏—Ç—Ä–æ–≤ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏?   \n",
       "\n",
       "                                              response classifier_1  \\\n",
       "143  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "144  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "145  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "146  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "147  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "148  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "149  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "150  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "151  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "152  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "153  –°–µ–π—á–∞—Å —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ ...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "854  –ü–æ–∫–∞ —Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ—Ç, –Ω–æ –º—ã —É—á—Ç–µ–º –≤–∞—à–µ –ø—Ä–µ–¥–ª...  –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø   \n",
       "\n",
       "    classifier_2 is_knowledge_base  labels  \n",
       "143        –ü–ª–µ–µ—Ä                 0      20  \n",
       "144        –ü–ª–µ–µ—Ä                 0      20  \n",
       "145        –ü–ª–µ–µ—Ä                 0      20  \n",
       "146        –ü–ª–µ–µ—Ä                 0      20  \n",
       "147        –ü–ª–µ–µ—Ä                 0      20  \n",
       "148        –ü–ª–µ–µ—Ä                 0      20  \n",
       "149        –ü–ª–µ–µ—Ä                 0      20  \n",
       "150        –ü–ª–µ–µ—Ä                 0      20  \n",
       "151        –ü–ª–µ–µ—Ä                 0      20  \n",
       "152        –ü–ª–µ–µ—Ä                 0      20  \n",
       "153        –ü–ª–µ–µ—Ä                 0      20  \n",
       "854        –ü–ª–µ–µ—Ä                 0      20  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['classifier_2'] == '–ü–ª–µ–µ—Ä'][:100-88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in least_classes.items():\n",
    "    samples = train_df[train_df['classifier_2'] == k]\n",
    "    if v == 55:\n",
    "        new = pd.concat([samples, samples])\n",
    "    elif v > 55:\n",
    "        new = pd.concat([samples, samples[:110-len(samples)]])\n",
    "    elif v < 55:\n",
    "        new = pd.concat([samples, samples])\n",
    "        while len(new) < 110:\n",
    "            new = pd.concat([new, samples])\n",
    "        else:\n",
    "            new = pd.concat([new, samples[:110-v]])\n",
    "    train_df = train_df[train_df['classifier_2'] != k]\n",
    "    train_df = pd.concat([train_df, new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier_2\n",
       "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è                            1212\n",
       "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–µ–π                     949\n",
       "–ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ                             898\n",
       "–û—Ç–∫–ª—é—á–µ–Ω–∏–µ/–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏         690\n",
       "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è/–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è                    561\n",
       "–û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ/–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –≤–∏–¥–µ–æ                503\n",
       "–°—Ç—É–¥–∏—è RUTUBE                              451\n",
       "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ                                 286\n",
       "–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è                                275\n",
       "–í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ                          226\n",
       "–ú–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è                                198\n",
       "–¢—Ä–∞–Ω—Å–ª—è—Ü–∏—è                                 176\n",
       "–ü–ª–∞—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç                            176\n",
       "–ò—Å—Ç–æ—Ä–∏—è –ø–æ–∏—Å–∫–∞                             165\n",
       "–ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç                        165\n",
       "–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–∏–¥–µ–æ                      132\n",
       "–ü—Ä–æ—Å–º–æ—Ç—Ä —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏                        132\n",
       "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è 0                           132\n",
       "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏                                132\n",
       "–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∫–ª–∞–º—ã             132\n",
       "–ß–∞—Ç/–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏                            132\n",
       "–ê–Ω–∞–ª–∏—Ç–∏–∫–∞                                  132\n",
       "–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ                        132\n",
       "–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫                            132\n",
       "–¢–í-—ç—Ñ–∏—Ä—ã                                   132\n",
       "–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∫–∞–Ω–∞–ª–∞                          121\n",
       "–°–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π                       121\n",
       "–°–º–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏/–≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è     121\n",
       "–ü—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–∞                         121\n",
       "–ü–µ—Ä–µ–Ω–æ—Å –≤–∏–¥–µ–æ —Å Youtube                    110\n",
       "–ù–∞—Ä—É—à–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –ø—Ä–∞–≤                   110\n",
       "–î–æ–ª–≥–∞—è –º–æ–¥–µ—Ä–∞—Ü–∏—è                           110\n",
       "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–ª–µ–µ—Ä–æ–º                         110\n",
       "–ü–ª–µ–µ—Ä                                      110\n",
       "–£–¥–∞–ª–µ–Ω–∏–µ –∞–∫–∫–∞—É–Ω—Ç–∞                          110\n",
       "–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–Ω–∞—Ç–æ–≤             110\n",
       "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–∏                  110\n",
       "–ù–∞–≤–∏–≥–∞—Ü–∏—è                                  110\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['classifier_2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df, split=\"train\")\n",
    "val_ds = Dataset.from_pandas(val_df, split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds =  train_ds.remove_columns([\"response\", \"classifier_1\", \"classifier_2\", \"is_knowledge_base\", \"__index_level_0__\"])\n",
    "val_ds =  val_ds.remove_columns([\"response\", \"classifier_1\", \"classifier_2\", \"is_knowledge_base\", \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds =  train_ds.rename_column(\"question\", \"text\")\n",
    "val_ds =  val_ds.rename_column(\"question\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '—Ä–æ–ª–∏–∫–∏ –∏–∑ –∏–Ω–µ—Ç–∞ –∑–∞—â–∏—â–µ–Ω—ã –∞–≤—Ç–æ—Ä—Å–∫–∏–º –ø—Ä–∞–≤–æ–º?', 'labels': 14}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepvk/deberta-v1-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900f0164463b4b0990906ea83272e096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a712b4091b4ebfaf7e68d0e44498a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/168 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_ds.map(tokenize, batched=True)\n",
    "tokenized_val = val_ds.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_train.shuffle(seed=0)\n",
    "tokenized_val = tokenized_val.shuffle(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\")\n",
    "metric1 = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    f1 = metric.compute(predictions=predictions, references=labels,\n",
    "                         average=\"macro\")[\"f1\"]\n",
    "    f1_w = metric.compute(predictions=predictions, references=labels,\n",
    "                         average=\"weighted\")[\"f1\"]\n",
    "    accuracy = metric1.compute(predictions=predictions, references=labels)[\n",
    "        \"accuracy\"]\n",
    "    \n",
    "    return {\"f1\": f1,\n",
    "            \"f1_weighted\": f1_w,\n",
    "            \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at deepvk/deberta-v1-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"deepvk/deberta-v1-base\", num_labels=len(id2label), label2id=label2id, id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.8/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"classifier_logs\",\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.00001,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    warmup_steps=300,\n",
    "    report_to=\"tensorboard\",\n",
    "    seed=42,\n",
    "    use_cpu=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='912' max='912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [912/912 06:28, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.274600</td>\n",
       "      <td>3.829794</td>\n",
       "      <td>0.103209</td>\n",
       "      <td>0.115801</td>\n",
       "      <td>0.148810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.723700</td>\n",
       "      <td>5.186765</td>\n",
       "      <td>0.161074</td>\n",
       "      <td>0.191385</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>6.215043</td>\n",
       "      <td>0.171865</td>\n",
       "      <td>0.194149</td>\n",
       "      <td>0.202381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>6.644105</td>\n",
       "      <td>0.176844</td>\n",
       "      <td>0.209823</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>7.377625</td>\n",
       "      <td>0.153418</td>\n",
       "      <td>0.211968</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>7.548243</td>\n",
       "      <td>0.155661</td>\n",
       "      <td>0.210585</td>\n",
       "      <td>0.220238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/alex/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=912, training_loss=0.7069190523685202, metrics={'train_runtime': 393.0604, 'train_samples_per_second': 148.45, 'train_steps_per_second': 2.32, 'total_flos': 3838960122854400.0, 'train_loss': 0.7069190523685202, 'epoch': 6.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b595f8625d396a8e1a27169e1f3cf13ad770e2c72392df9ca9f9b56bdad2d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
